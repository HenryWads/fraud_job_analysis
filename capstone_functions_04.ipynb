{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d72453-0620-445c-a492-1e0448794994",
   "metadata": {},
   "source": [
    "## Functions Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f655773a-24ca-4344-86fd-2b931d8cf506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import category_encoders as ce \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05eecc7-6b66-4725-b7ed-693aa7418a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image tag\n",
    "imageTag = [f'image{i:03d}' for i in range(200)]\n",
    "\n",
    "# function that saves visualisation plots\n",
    "def save_plot_function(imageRef, folder=r'C:\\Users\\hjame\\Documents\\Data Science\\Mini Projects\\capstoneProject\\images'):\n",
    "    filename = f'{imageTag[imageRef]}.png'\n",
    "    path = os.path.join(folder, filename)\n",
    "    plt.savefig(path, facecolor = 'w', bbox_inches='tight',pad_inches=0.3, transparent=True)\n",
    "    plt.show()\n",
    "    print(f'Plot saved to: {path}')\n",
    "    return\n",
    "\n",
    "# create function to show how the target variable varies based on other columns\n",
    "def target_distribution_barplot(dataframe, x_feature, y_feature, imageNum, palette='Set1'):\n",
    "    # plt.figure(figsize=(14,8))\n",
    "    sns.barplot(data=dataframe, \n",
    "                x=x_feature, \n",
    "                y=y_feature, \n",
    "                palette=palette, \n",
    "                estimator=sum,\n",
    "                errorbar=None,\n",
    "                hue=x_feature, \n",
    "                legend=False ).set(\n",
    "    title=f'Fraudulent Job Postings by {x_feature}')\n",
    "    plt.ylabel('Total of Fraudulent Job Postings')\n",
    "    plt.xticks(rotation=45)\n",
    "    save_plot_function(imageNum)\n",
    "    return\n",
    "\n",
    "# create function to identify text that is not english\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "# text preprocessing function\n",
    "# convert to lowercase, remove text in sqaure brackets, remove links, remove punctuation, remove words containing numbers \n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Lowercase\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'http\\S+|https\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemma.lemmatize(word) for word in words if word not in stop_words]\n",
    "\n",
    "    return ' '.join(words)  \n",
    "\n",
    "# learning curve function for three models \n",
    "\n",
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "# Word2Vec function \n",
    "# convert each document into single vector by averaging word emeddings\n",
    "\n",
    "def get_average_word2vec(tokens, model, vector_size):\n",
    "    valid_tokens = [token for token in tokens if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(model.wv[valid_tokens], axis=0)\n",
    "\n",
    "# learning curve function \n",
    "def learning_curve_function(model, X, y, imageNum, name='Model', cv=5, scoring='accuracy'):\n",
    "    # Get learning curve data\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        n_jobs=-1,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='green', label='Training Score')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='green')\n",
    "\n",
    "    plt.plot(train_sizes, test_mean, 'o-', color='red', label='Cross-validation Score')\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')\n",
    "\n",
    "    plt.title(f'Learning Curve: {name}', fontsize=14)\n",
    "    plt.xlabel('Training Set Size', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "\n",
    "    # Save figure\n",
    "    save_plot_function(imageNum)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# # Randomized hyper parameter tuning function \n",
    "\n",
    "# def hyper_tune_evalutation_model(model, param_dist, X_train, X_test, y_train, y_test, model_name, vectorizer_name, n_iter=20, cv=5):\n",
    "#     random_search = RandomizedSearchCV(\n",
    "#         estimator=model,\n",
    "#         param_distribution,\n",
    "#         n_iter=n_iter,\n",
    "#         cv=cv,\n",
    "#         scoring='accuracy',\n",
    "#         n_job=-1,\n",
    "#         random_state=42,\n",
    "#     )\n",
    "\n",
    "#     random_search.fit(X_train, y_train)\n",
    "#     best_model = random_search.best_estimator_\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     roc = roc_auc_score(y_test, best_model_predict_proba(X_test)[:,1])\n",
    "\n",
    "#     results.append({\n",
    "#     'Model': model_name,\n",
    "#     'Vectorizer': Vectorizer_name,\n",
    "#     'Best_Params': random_search.best_params,\n",
    "#     'Test Accuracy': best_model.score(X_test, y_test),\n",
    "#     'ROC AUC': roc\n",
    "#     })\n",
    "\n",
    "#     print(f'Model: {model_name}')\n",
    "#     print(f'Vectorizer: {vectorizer_name}')\n",
    "#     print(f'Best Parameters: {random_search.best_params_}')\n",
    "#     print(f'Confusion Matrix:\\n, {confusion_matrix(y_test, y_pred)}')\n",
    "#     print(f'Classification Report:\\n, {classification_report(y_test, y_pred)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b684a-e0b2-4263-a7da-8be4fb8dcee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b2cba-4f81-4427-8a25-b377461e54d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4091f-9f40-4e17-ae26-9d3af2d1e695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4f0e5-c15f-40d6-8567-b63e355dbf02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0174153f-1904-4f6d-ad2a-b5651960b32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7540c-340d-41df-bc71-7be87362e8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57ba0f-2a31-41f6-bcb7-21875a1aa9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfea758-b0c2-411c-adf4-0a0dfd66eff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0d085-24a6-4da6-8c3d-c1350bcd779a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b7cad-bea3-46ff-9233-387d32746acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4a446-e448-4bd2-b05f-81f5b10aa1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb646f65-dc10-4c4f-9e44-2f865e3d0610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e710f-2dd3-4ea8-83eb-e274477a8bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db425bec-c79e-4087-a512-dc07980eb145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a2809-ae47-4549-9913-a6cf3c08c732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25122b11-283b-441d-b359-26b588fde681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450bf73-3f27-4323-9d32-e961988db505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436fa3b1-5a60-42bd-a7ce-1be461dcdd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a337f-39f5-4d1e-9f54-871901f2bee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f01df06-4746-49ea-b694-3c366fb3aff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce7729-1c29-4b0a-bbcf-c675d5d440a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
